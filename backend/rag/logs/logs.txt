2025-08-17 15:37:07.0275 - INFO - graphrag.cli.index - Logging enabled at E:\ai\AI-Driven-TRPG\backend\rag\logs\logs.txt
2025-08-17 15:37:11.0087 - INFO - graphrag.index.validate_config - LLM Config Params Validated
2025-08-17 15:37:12.0032 - INFO - graphrag.index.validate_config - Embedding LLM Config Params Validated
2025-08-17 15:37:12.0032 - INFO - graphrag.cli.index - Starting pipeline run. False
2025-08-17 15:37:12.0032 - INFO - graphrag.cli.index - Using default configuration: {
    "root_dir": "E:\\ai\\AI-Driven-TRPG\\backend\\rag",
    "models": {
        "default_chat_model": {
            "api_key": "==== REDACTED ====",
            "auth_type": "api_key",
            "type": "openai_chat",
            "model": "gpt-4-turbo-preview",
            "encoding_model": "cl100k_base",
            "api_base": null,
            "api_version": null,
            "deployment_name": null,
            "proxy": null,
            "audience": null,
            "model_supports_json": true,
            "request_timeout": 180.0,
            "tokens_per_minute": "auto",
            "requests_per_minute": "auto",
            "retry_strategy": "native",
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "concurrent_requests": 25,
            "async_mode": "threaded",
            "responses": null,
            "max_tokens": null,
            "temperature": 0,
            "max_completion_tokens": null,
            "reasoning_effort": null,
            "top_p": 1,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0
        },
        "default_embedding_model": {
            "api_key": "==== REDACTED ====",
            "auth_type": "api_key",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "encoding_model": "cl100k_base",
            "api_base": null,
            "api_version": null,
            "deployment_name": null,
            "proxy": null,
            "audience": null,
            "model_supports_json": true,
            "request_timeout": 180.0,
            "tokens_per_minute": null,
            "requests_per_minute": null,
            "retry_strategy": "native",
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "concurrent_requests": 25,
            "async_mode": "threaded",
            "responses": null,
            "max_tokens": null,
            "temperature": 0,
            "max_completion_tokens": null,
            "reasoning_effort": null,
            "top_p": 1,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0
        }
    },
    "input": {
        "storage": {
            "type": "file",
            "base_dir": "E:\\ai\\AI-Driven-TRPG\\backend\\rag\\input",
            "storage_account_blob_url": null,
            "cosmosdb_account_url": null
        },
        "file_type": "text",
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "text_column": "text",
        "title_column": null,
        "metadata": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": "tokens",
        "encoding_model": "cl100k_base",
        "prepend_metadata": false,
        "chunk_size_includes_metadata": false
    },
    "output": {
        "type": "file",
        "base_dir": "E:\\ai\\AI-Driven-TRPG\\backend\\rag\\output",
        "storage_account_blob_url": null,
        "cosmosdb_account_url": null
    },
    "outputs": null,
    "update_index_output": {
        "type": "file",
        "base_dir": "E:\\ai\\AI-Driven-TRPG\\backend\\rag\\update_output",
        "storage_account_blob_url": null,
        "cosmosdb_account_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null,
        "cosmosdb_account_url": null
    },
    "reporting": {
        "type": "file",
        "base_dir": "E:\\ai\\AI-Driven-TRPG\\backend\\rag\\logs",
        "storage_account_blob_url": null
    },
    "vector_store": {
        "default_vector_store": {
            "type": "lancedb",
            "db_uri": "E:\\ai\\AI-Driven-TRPG\\backend\\rag\\output\\lancedb",
            "url": null,
            "audience": null,
            "container_name": "==== REDACTED ====",
            "database_name": null,
            "overwrite": true
        }
    },
    "workflows": null,
    "embed_text": {
        "model_id": "default_embedding_model",
        "vector_store_id": "default_vector_store",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "names": [
            "entity.description",
            "community.full_content",
            "text_unit.text"
        ],
        "strategy": null
    },
    "extract_graph": {
        "model_id": "default_chat_model",
        "prompt": "prompts/extract_graph.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null
    },
    "summarize_descriptions": {
        "model_id": "default_chat_model",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "max_input_tokens": 4000,
        "strategy": null
    },
    "extract_graph_nlp": {
        "normalize_edge_weights": true,
        "text_analyzer": {
            "extractor_type": "regex_english",
            "model_name": "en_core_web_md",
            "max_word_length": 15,
            "word_delimiter": " ",
            "include_named_entities": true,
            "exclude_nouns": [
                "stuff",
                "thing",
                "things",
                "bunch",
                "bit",
                "bits",
                "people",
                "person",
                "okay",
                "hey",
                "hi",
                "hello",
                "laughter",
                "oh"
            ],
            "exclude_entity_tags": [
                "DATE"
            ],
            "exclude_pos_tags": [
                "DET",
                "PRON",
                "INTJ",
                "X"
            ],
            "noun_phrase_tags": [
                "PROPN",
                "NOUNS"
            ],
            "noun_phrase_grammars": {
                "PROPN,PROPN": "PROPN",
                "NOUN,NOUN": "NOUNS",
                "NOUNS,NOUN": "NOUNS",
                "ADJ,ADJ": "ADJ",
                "ADJ,NOUN": "NOUNS"
            }
        },
        "concurrent_requests": 25
    },
    "prune_graph": {
        "min_node_freq": 2,
        "max_node_freq_std": null,
        "min_node_degree": 1,
        "max_node_degree_std": null,
        "min_edge_weight_pct": 40.0,
        "remove_ego_nodes": true,
        "lcc_only": false
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "use_lcc": true,
        "seed": 3735928559
    },
    "extract_claims": {
        "enabled": false,
        "model_id": "default_chat_model",
        "prompt": "prompts/extract_claims.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null
    },
    "community_reports": {
        "model_id": "default_chat_model",
        "graph_prompt": "prompts/community_report_graph.txt",
        "text_prompt": "prompts/community_report_text.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "embed_graph": {
        "enabled": false,
        "dimensions": 1536,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "use_lcc": true
    },
    "umap": {
        "enabled": false
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false,
        "raw_graph": false
    },
    "local_search": {
        "prompt": "prompts/local_search_system_prompt.txt",
        "chat_model_id": "default_chat_model",
        "embedding_model_id": "default_embedding_model",
        "text_unit_prop": 0.5,
        "community_prop": 0.15,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "max_context_tokens": 12000
    },
    "global_search": {
        "map_prompt": "prompts/global_search_map_system_prompt.txt",
        "reduce_prompt": "prompts/global_search_reduce_system_prompt.txt",
        "chat_model_id": "default_chat_model",
        "knowledge_prompt": "prompts/global_search_knowledge_system_prompt.txt",
        "max_context_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_length": 1000,
        "reduce_max_length": 2000,
        "dynamic_search_threshold": 1,
        "dynamic_search_keep_parent": false,
        "dynamic_search_num_repeats": 1,
        "dynamic_search_use_summary": false,
        "dynamic_search_max_level": 2
    },
    "drift_search": {
        "prompt": "prompts/drift_search_system_prompt.txt",
        "reduce_prompt": "prompts/drift_search_reduce_prompt.txt",
        "chat_model_id": "default_chat_model",
        "embedding_model_id": "default_embedding_model",
        "data_max_tokens": 12000,
        "reduce_max_tokens": null,
        "reduce_temperature": 0,
        "reduce_max_completion_tokens": null,
        "concurrency": 32,
        "drift_k_followups": 20,
        "primer_folds": 5,
        "primer_llm_max_tokens": 12000,
        "n_depth": 3,
        "local_search_text_unit_prop": 0.9,
        "local_search_community_prop": 0.1,
        "local_search_top_k_mapped_entities": 10,
        "local_search_top_k_relationships": 10,
        "local_search_max_data_tokens": 12000,
        "local_search_temperature": 0,
        "local_search_top_p": 1,
        "local_search_n": 1,
        "local_search_llm_max_gen_tokens": null,
        "local_search_llm_max_gen_completion_tokens": null
    },
    "basic_search": {
        "prompt": "prompts/basic_search_system_prompt.txt",
        "chat_model_id": "default_chat_model",
        "embedding_model_id": "default_embedding_model",
        "k": 10,
        "max_context_tokens": 12000
    }
}
2025-08-17 15:37:12.0038 - INFO - graphrag.api.index - Initializing indexing pipeline...
2025-08-17 15:37:12.0038 - INFO - graphrag.index.workflows.factory - Creating pipeline with workflows: ['load_input_documents', 'create_base_text_units', 'create_final_documents', 'extract_graph', 'finalize_graph', 'extract_covariates', 'create_communities', 'create_final_text_units', 'create_community_reports', 'generate_text_embeddings']
2025-08-17 15:37:12.0038 - INFO - graphrag.storage.file_pipeline_storage - Creating file storage at E:\ai\AI-Driven-TRPG\backend\rag\input
2025-08-17 15:37:12.0038 - INFO - graphrag.storage.file_pipeline_storage - Creating file storage at E:\ai\AI-Driven-TRPG\backend\rag\output
2025-08-17 15:37:12.0041 - INFO - graphrag.index.run.run_pipeline - Running standard indexing.
2025-08-17 15:37:12.0043 - INFO - graphrag.index.run.run_pipeline - Executing pipeline...
2025-08-17 15:37:12.0043 - INFO - graphrag.index.input.factory - loading input from root_dir=E:\ai\AI-Driven-TRPG\backend\rag\input
2025-08-17 15:37:12.0043 - INFO - graphrag.index.input.factory - Loading Input InputFileType.text
2025-08-17 15:37:12.0043 - INFO - graphrag.storage.file_pipeline_storage - search E:\ai\AI-Driven-TRPG\backend\rag\input for files matching .*\.txt$
2025-08-17 15:37:12.0045 - WARNING - graphrag.index.input.util - Warning! Error loading file dead_light.txt. Skipping...
2025-08-17 15:37:12.0045 - WARNING - graphrag.index.input.util - Error: 'utf-8' codec can't decode byte 0xcb in position 0: invalid continuation byte
2025-08-17 15:37:12.0045 - INFO - graphrag.index.input.util - Found 1 InputFileType.text files, loading 0
2025-08-17 15:37:12.0045 - ERROR - graphrag.index.run.run_pipeline - error running workflow load_input_documents
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\run\run_pipeline.py", line 108, in _run_pipeline
    result = await workflow_function(config, context)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\workflows\load_input_documents.py", line 26, in run_workflow
    output = await load_input_documents(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\workflows\load_input_documents.py", line 43, in load_input_documents
    return await create_input(config, storage)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\input\factory.py", line 37, in create_input
    result = await loader(config, storage)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\input\text.py", line 35, in load_text
    return await load_files(load_file, config, storage)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\input\util.py", line 48, in load_files
    result = pd.concat(files_loaded)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\reshape\concat.py", line 382, in concat
    op = _Concatenator(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\reshape\concat.py", line 445, in __init__
    objs, keys = self._clean_keys_and_objs(objs, keys)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\reshape\concat.py", line 507, in _clean_keys_and_objs
    raise ValueError("No objects to concatenate")
ValueError: No objects to concatenate
2025-08-17 15:37:12.0046 - ERROR - graphrag.api.index - Workflow load_input_documents completed with errors
2025-08-17 15:37:12.0046 - ERROR - graphrag.cli.index - Errors occurred during the pipeline run, see logs for more details.
2025-08-17 15:38:46.0171 - INFO - graphrag.cli.index - Logging enabled at E:\ai\AI-Driven-TRPG\backend\rag\logs\logs.txt
2025-08-17 15:38:48.0146 - INFO - graphrag.index.validate_config - LLM Config Params Validated
2025-08-17 15:38:49.0042 - INFO - graphrag.index.validate_config - Embedding LLM Config Params Validated
2025-08-17 15:38:49.0042 - INFO - graphrag.cli.index - Starting pipeline run. False
2025-08-17 15:38:49.0042 - INFO - graphrag.cli.index - Using default configuration: {
    "root_dir": "E:\\ai\\AI-Driven-TRPG\\backend\\rag",
    "models": {
        "default_chat_model": {
            "api_key": "==== REDACTED ====",
            "auth_type": "api_key",
            "type": "openai_chat",
            "model": "gpt-4-turbo-preview",
            "encoding_model": "cl100k_base",
            "api_base": null,
            "api_version": null,
            "deployment_name": null,
            "proxy": null,
            "audience": null,
            "model_supports_json": true,
            "request_timeout": 180.0,
            "tokens_per_minute": "auto",
            "requests_per_minute": "auto",
            "retry_strategy": "native",
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "concurrent_requests": 25,
            "async_mode": "threaded",
            "responses": null,
            "max_tokens": null,
            "temperature": 0,
            "max_completion_tokens": null,
            "reasoning_effort": null,
            "top_p": 1,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0
        },
        "default_embedding_model": {
            "api_key": "==== REDACTED ====",
            "auth_type": "api_key",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "encoding_model": "cl100k_base",
            "api_base": null,
            "api_version": null,
            "deployment_name": null,
            "proxy": null,
            "audience": null,
            "model_supports_json": true,
            "request_timeout": 180.0,
            "tokens_per_minute": null,
            "requests_per_minute": null,
            "retry_strategy": "native",
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "concurrent_requests": 25,
            "async_mode": "threaded",
            "responses": null,
            "max_tokens": null,
            "temperature": 0,
            "max_completion_tokens": null,
            "reasoning_effort": null,
            "top_p": 1,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0
        }
    },
    "input": {
        "storage": {
            "type": "file",
            "base_dir": "E:\\ai\\AI-Driven-TRPG\\backend\\rag\\input",
            "storage_account_blob_url": null,
            "cosmosdb_account_url": null
        },
        "file_type": "text",
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "text_column": "text",
        "title_column": null,
        "metadata": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": "tokens",
        "encoding_model": "cl100k_base",
        "prepend_metadata": false,
        "chunk_size_includes_metadata": false
    },
    "output": {
        "type": "file",
        "base_dir": "E:\\ai\\AI-Driven-TRPG\\backend\\rag\\output",
        "storage_account_blob_url": null,
        "cosmosdb_account_url": null
    },
    "outputs": null,
    "update_index_output": {
        "type": "file",
        "base_dir": "E:\\ai\\AI-Driven-TRPG\\backend\\rag\\update_output",
        "storage_account_blob_url": null,
        "cosmosdb_account_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null,
        "cosmosdb_account_url": null
    },
    "reporting": {
        "type": "file",
        "base_dir": "E:\\ai\\AI-Driven-TRPG\\backend\\rag\\logs",
        "storage_account_blob_url": null
    },
    "vector_store": {
        "default_vector_store": {
            "type": "lancedb",
            "db_uri": "E:\\ai\\AI-Driven-TRPG\\backend\\rag\\output\\lancedb",
            "url": null,
            "audience": null,
            "container_name": "==== REDACTED ====",
            "database_name": null,
            "overwrite": true
        }
    },
    "workflows": null,
    "embed_text": {
        "model_id": "default_embedding_model",
        "vector_store_id": "default_vector_store",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "names": [
            "entity.description",
            "community.full_content",
            "text_unit.text"
        ],
        "strategy": null
    },
    "extract_graph": {
        "model_id": "default_chat_model",
        "prompt": "prompts/extract_graph.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null
    },
    "summarize_descriptions": {
        "model_id": "default_chat_model",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "max_input_tokens": 4000,
        "strategy": null
    },
    "extract_graph_nlp": {
        "normalize_edge_weights": true,
        "text_analyzer": {
            "extractor_type": "regex_english",
            "model_name": "en_core_web_md",
            "max_word_length": 15,
            "word_delimiter": " ",
            "include_named_entities": true,
            "exclude_nouns": [
                "stuff",
                "thing",
                "things",
                "bunch",
                "bit",
                "bits",
                "people",
                "person",
                "okay",
                "hey",
                "hi",
                "hello",
                "laughter",
                "oh"
            ],
            "exclude_entity_tags": [
                "DATE"
            ],
            "exclude_pos_tags": [
                "DET",
                "PRON",
                "INTJ",
                "X"
            ],
            "noun_phrase_tags": [
                "PROPN",
                "NOUNS"
            ],
            "noun_phrase_grammars": {
                "PROPN,PROPN": "PROPN",
                "NOUN,NOUN": "NOUNS",
                "NOUNS,NOUN": "NOUNS",
                "ADJ,ADJ": "ADJ",
                "ADJ,NOUN": "NOUNS"
            }
        },
        "concurrent_requests": 25
    },
    "prune_graph": {
        "min_node_freq": 2,
        "max_node_freq_std": null,
        "min_node_degree": 1,
        "max_node_degree_std": null,
        "min_edge_weight_pct": 40.0,
        "remove_ego_nodes": true,
        "lcc_only": false
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "use_lcc": true,
        "seed": 3735928559
    },
    "extract_claims": {
        "enabled": false,
        "model_id": "default_chat_model",
        "prompt": "prompts/extract_claims.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null
    },
    "community_reports": {
        "model_id": "default_chat_model",
        "graph_prompt": "prompts/community_report_graph.txt",
        "text_prompt": "prompts/community_report_text.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "embed_graph": {
        "enabled": false,
        "dimensions": 1536,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "use_lcc": true
    },
    "umap": {
        "enabled": false
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false,
        "raw_graph": false
    },
    "local_search": {
        "prompt": "prompts/local_search_system_prompt.txt",
        "chat_model_id": "default_chat_model",
        "embedding_model_id": "default_embedding_model",
        "text_unit_prop": 0.5,
        "community_prop": 0.15,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "max_context_tokens": 12000
    },
    "global_search": {
        "map_prompt": "prompts/global_search_map_system_prompt.txt",
        "reduce_prompt": "prompts/global_search_reduce_system_prompt.txt",
        "chat_model_id": "default_chat_model",
        "knowledge_prompt": "prompts/global_search_knowledge_system_prompt.txt",
        "max_context_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_length": 1000,
        "reduce_max_length": 2000,
        "dynamic_search_threshold": 1,
        "dynamic_search_keep_parent": false,
        "dynamic_search_num_repeats": 1,
        "dynamic_search_use_summary": false,
        "dynamic_search_max_level": 2
    },
    "drift_search": {
        "prompt": "prompts/drift_search_system_prompt.txt",
        "reduce_prompt": "prompts/drift_search_reduce_prompt.txt",
        "chat_model_id": "default_chat_model",
        "embedding_model_id": "default_embedding_model",
        "data_max_tokens": 12000,
        "reduce_max_tokens": null,
        "reduce_temperature": 0,
        "reduce_max_completion_tokens": null,
        "concurrency": 32,
        "drift_k_followups": 20,
        "primer_folds": 5,
        "primer_llm_max_tokens": 12000,
        "n_depth": 3,
        "local_search_text_unit_prop": 0.9,
        "local_search_community_prop": 0.1,
        "local_search_top_k_mapped_entities": 10,
        "local_search_top_k_relationships": 10,
        "local_search_max_data_tokens": 12000,
        "local_search_temperature": 0,
        "local_search_top_p": 1,
        "local_search_n": 1,
        "local_search_llm_max_gen_tokens": null,
        "local_search_llm_max_gen_completion_tokens": null
    },
    "basic_search": {
        "prompt": "prompts/basic_search_system_prompt.txt",
        "chat_model_id": "default_chat_model",
        "embedding_model_id": "default_embedding_model",
        "k": 10,
        "max_context_tokens": 12000
    }
}
2025-08-17 15:38:49.0046 - INFO - graphrag.api.index - Initializing indexing pipeline...
2025-08-17 15:38:49.0046 - INFO - graphrag.index.workflows.factory - Creating pipeline with workflows: ['load_input_documents', 'create_base_text_units', 'create_final_documents', 'extract_graph', 'finalize_graph', 'extract_covariates', 'create_communities', 'create_final_text_units', 'create_community_reports', 'generate_text_embeddings']
2025-08-17 15:38:49.0046 - INFO - graphrag.storage.file_pipeline_storage - Creating file storage at E:\ai\AI-Driven-TRPG\backend\rag\input
2025-08-17 15:38:49.0046 - INFO - graphrag.storage.file_pipeline_storage - Creating file storage at E:\ai\AI-Driven-TRPG\backend\rag\output
2025-08-17 15:38:49.0046 - INFO - graphrag.index.run.run_pipeline - Running standard indexing.
2025-08-17 15:38:49.0051 - INFO - graphrag.index.run.run_pipeline - Executing pipeline...
2025-08-17 15:38:49.0051 - INFO - graphrag.index.input.factory - loading input from root_dir=E:\ai\AI-Driven-TRPG\backend\rag\input
2025-08-17 15:38:49.0051 - INFO - graphrag.index.input.factory - Loading Input InputFileType.text
2025-08-17 15:38:49.0051 - INFO - graphrag.storage.file_pipeline_storage - search E:\ai\AI-Driven-TRPG\backend\rag\input for files matching .*\.txt$
2025-08-17 15:38:49.0053 - INFO - graphrag.index.input.util - Found 1 InputFileType.text files, loading 1
2025-08-17 15:38:49.0053 - INFO - graphrag.index.input.util - Total number of unfiltered text rows: 1
2025-08-17 15:38:49.0053 - INFO - graphrag.index.workflows.load_input_documents - Final # of rows loaded: 1
2025-08-17 15:38:49.0058 - INFO - graphrag.api.index - Workflow load_input_documents completed successfully
2025-08-17 15:38:49.0064 - INFO - graphrag.index.workflows.create_base_text_units - Workflow started: create_base_text_units
2025-08-17 15:38:49.0064 - INFO - graphrag.utils.storage - reading table from storage: documents.parquet
2025-08-17 15:38:49.0073 - INFO - graphrag.index.workflows.create_base_text_units - Starting chunking process for 1 documents
2025-08-17 15:38:49.0086 - INFO - graphrag.index.workflows.create_base_text_units - chunker progress:  1/1
2025-08-17 15:38:49.0092 - INFO - graphrag.index.workflows.create_base_text_units - Workflow completed: create_base_text_units
2025-08-17 15:38:49.0092 - INFO - graphrag.api.index - Workflow create_base_text_units completed successfully
2025-08-17 15:38:49.0096 - INFO - graphrag.index.workflows.create_final_documents - Workflow started: create_final_documents
2025-08-17 15:38:49.0099 - INFO - graphrag.utils.storage - reading table from storage: documents.parquet
2025-08-17 15:38:49.0101 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2025-08-17 15:38:49.0117 - INFO - graphrag.index.workflows.create_final_documents - Workflow completed: create_final_documents
2025-08-17 15:38:49.0117 - INFO - graphrag.api.index - Workflow create_final_documents completed successfully
2025-08-17 15:38:49.0122 - INFO - graphrag.index.workflows.extract_graph - Workflow started: extract_graph
2025-08-17 15:38:49.0123 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2025-08-17 15:39:05.0146 - INFO - graphrag.logger.progress - extract graph progress: 1/24
2025-08-17 15:39:39.0816 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 1747. Please try again in 3.494s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:39:39.0822 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 1747. Please try again in 3.494s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:39:39.0822 - INFO - graphrag.logger.progress - extract graph progress: 2/24
2025-08-17 15:39:53.0243 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2397. Please try again in 4.794s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:39:53.0245 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2397. Please try again in 4.794s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:39:53.0245 - INFO - graphrag.logger.progress - extract graph progress: 3/24
2025-08-17 15:39:59.0039 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2485. Please try again in 4.97s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:39:59.0039 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2485. Please try again in 4.97s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:39:59.0039 - INFO - graphrag.logger.progress - extract graph progress: 4/24
2025-08-17 15:39:59.0086 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2546. Please try again in 5.092s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:39:59.0086 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2546. Please try again in 5.092s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:39:59.0089 - INFO - graphrag.logger.progress - extract graph progress: 5/24
2025-08-17 15:40:00.0957 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2551. Please try again in 5.102s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:40:00.0960 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2551. Please try again in 5.102s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:40:00.0960 - INFO - graphrag.logger.progress - extract graph progress: 6/24
2025-08-17 15:40:01.0120 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2583. Please try again in 5.166s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:40:01.0120 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2583. Please try again in 5.166s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:40:01.0120 - INFO - graphrag.logger.progress - extract graph progress: 7/24
2025-08-17 15:40:01.0172 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2467. Please try again in 4.934s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:40:01.0174 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2467. Please try again in 4.934s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:40:01.0174 - INFO - graphrag.logger.progress - extract graph progress: 8/24
2025-08-17 15:40:01.0375 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2519. Please try again in 5.038s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:40:01.0375 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2519. Please try again in 5.038s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:40:01.0375 - INFO - graphrag.logger.progress - extract graph progress: 9/24
2025-08-17 15:40:01.0780 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2549. Please try again in 5.098s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:40:01.0780 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2549. Please try again in 5.098s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:40:01.0780 - INFO - graphrag.logger.progress - extract graph progress: 10/24
2025-08-17 15:40:01.0841 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2541. Please try again in 5.082s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:40:01.0841 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2541. Please try again in 5.082s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:40:01.0841 - INFO - graphrag.logger.progress - extract graph progress: 11/24
2025-08-17 15:40:02.0055 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2469. Please try again in 4.938s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:40:02.0056 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2469. Please try again in 4.938s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:40:02.0057 - INFO - graphrag.logger.progress - extract graph progress: 12/24
2025-08-17 15:40:02.0459 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2575. Please try again in 5.15s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:40:02.0459 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2575. Please try again in 5.15s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:40:02.0459 - INFO - graphrag.logger.progress - extract graph progress: 13/24
2025-08-17 15:40:04.0249 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2562. Please try again in 5.124s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:40:04.0249 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2562. Please try again in 5.124s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:40:04.0251 - INFO - graphrag.logger.progress - extract graph progress: 14/24
2025-08-17 15:40:04.0461 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2548. Please try again in 5.096s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:40:04.0463 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2548. Please try again in 5.096s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:40:04.0464 - INFO - graphrag.logger.progress - extract graph progress: 15/24
2025-08-17 15:40:04.0880 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2612. Please try again in 5.224s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:40:04.0880 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2612. Please try again in 5.224s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:40:04.0880 - INFO - graphrag.logger.progress - extract graph progress: 16/24
2025-08-17 15:40:04.0978 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2560. Please try again in 5.12s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:40:04.0978 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2560. Please try again in 5.12s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:40:04.0978 - INFO - graphrag.logger.progress - extract graph progress: 17/24
2025-08-17 15:40:05.0978 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2675. Please try again in 5.35s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:40:05.0978 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2675. Please try again in 5.35s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:40:05.0978 - INFO - graphrag.logger.progress - extract graph progress: 18/24
2025-08-17 15:40:06.0545 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2544. Please try again in 5.088s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:40:06.0545 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2544. Please try again in 5.088s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:40:06.0545 - INFO - graphrag.logger.progress - extract graph progress: 19/24
2025-08-17 15:40:06.0735 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2536. Please try again in 5.072s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:40:06.0736 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2536. Please try again in 5.072s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:40:06.0736 - INFO - graphrag.logger.progress - extract graph progress: 20/24
2025-08-17 15:40:06.0867 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2582. Please try again in 5.164s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:40:06.0867 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2582. Please try again in 5.164s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:40:06.0867 - INFO - graphrag.logger.progress - extract graph progress: 21/24
2025-08-17 15:40:13.0504 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2681. Please try again in 5.362s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:40:13.0506 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2681. Please try again in 5.362s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:40:13.0506 - INFO - graphrag.logger.progress - extract graph progress: 22/24
2025-08-17 15:40:19.0745 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2796. Please try again in 5.592s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:40:19.0745 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2796. Please try again in 5.592s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:40:19.0745 - INFO - graphrag.logger.progress - extract graph progress: 23/24
2025-08-17 15:40:22.0353 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2816. Please try again in 5.632s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:40:22.0355 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 2816. Please try again in 5.632s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 15:40:22.0355 - INFO - graphrag.logger.progress - extract graph progress: 24/24
2025-08-17 15:40:22.0385 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 1/10
2025-08-17 15:40:22.0386 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 2/10
2025-08-17 15:40:22.0386 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 3/10
2025-08-17 15:40:22.0386 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 4/10
2025-08-17 15:40:22.0386 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 5/10
2025-08-17 15:40:22.0386 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 6/10
2025-08-17 15:40:22.0387 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 7/10
2025-08-17 15:40:22.0387 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 8/10
2025-08-17 15:40:22.0387 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 9/10
2025-08-17 15:40:22.0387 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 10/10
2025-08-17 15:40:22.0397 - INFO - graphrag.index.workflows.extract_graph - Workflow completed: extract_graph
2025-08-17 15:40:22.0397 - INFO - graphrag.api.index - Workflow extract_graph completed successfully
2025-08-17 15:40:22.0405 - INFO - graphrag.index.workflows.finalize_graph - Workflow started: finalize_graph
2025-08-17 15:40:22.0405 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2025-08-17 15:40:22.0410 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2025-08-17 15:40:22.0425 - INFO - graphrag.index.workflows.finalize_graph - Workflow completed: finalize_graph
2025-08-17 15:40:22.0425 - INFO - graphrag.api.index - Workflow finalize_graph completed successfully
2025-08-17 15:40:22.0431 - INFO - graphrag.index.workflows.extract_covariates - Workflow started: extract_covariates
2025-08-17 15:40:22.0431 - INFO - graphrag.index.workflows.extract_covariates - Workflow completed: extract_covariates
2025-08-17 15:40:22.0431 - INFO - graphrag.api.index - Workflow extract_covariates completed successfully
2025-08-17 15:40:22.0432 - INFO - graphrag.index.workflows.create_communities - Workflow started: create_communities
2025-08-17 15:40:22.0432 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2025-08-17 15:40:22.0435 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2025-08-17 15:40:22.0457 - INFO - graphrag.index.workflows.create_communities - Workflow completed: create_communities
2025-08-17 15:40:22.0457 - INFO - graphrag.api.index - Workflow create_communities completed successfully
2025-08-17 15:40:22.0462 - INFO - graphrag.index.workflows.create_final_text_units - Workflow started: create_final_text_units
2025-08-17 15:40:22.0462 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2025-08-17 15:40:22.0466 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2025-08-17 15:40:22.0469 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2025-08-17 15:40:22.0483 - INFO - graphrag.index.workflows.create_final_text_units - Workflow completed: create_final_text_units
2025-08-17 15:40:22.0483 - INFO - graphrag.api.index - Workflow create_final_text_units completed successfully
2025-08-17 15:40:22.0491 - INFO - graphrag.index.workflows.create_community_reports - Workflow started: create_community_reports
2025-08-17 15:40:22.0492 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2025-08-17 15:40:22.0496 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2025-08-17 15:40:22.0500 - INFO - graphrag.utils.storage - reading table from storage: communities.parquet
2025-08-17 15:40:22.0511 - INFO - graphrag.index.operations.summarize_communities.graph_context.context_builder - Number of nodes at level=0 => 4
2025-08-17 15:41:02.0989 - INFO - graphrag.logger.progress - level 0 summarize communities progress: 1/1
2025-08-17 15:41:02.0995 - INFO - graphrag.index.workflows.create_community_reports - Workflow completed: create_community_reports
2025-08-17 15:41:02.0995 - INFO - graphrag.api.index - Workflow create_community_reports completed successfully
2025-08-17 15:41:03.0003 - INFO - graphrag.index.workflows.generate_text_embeddings - Workflow started: generate_text_embeddings
2025-08-17 15:41:03.0004 - INFO - graphrag.utils.storage - reading table from storage: documents.parquet
2025-08-17 15:41:03.0007 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2025-08-17 15:41:03.0010 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2025-08-17 15:41:03.0014 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2025-08-17 15:41:03.0017 - INFO - graphrag.utils.storage - reading table from storage: community_reports.parquet
2025-08-17 15:41:03.0024 - INFO - graphrag.index.workflows.generate_text_embeddings - Creating embeddings
2025-08-17 15:41:03.0024 - INFO - graphrag.index.operations.embed_text.embed_text - using vector store lancedb with container_name default for embedding entity.description: default-entity-description
2025-08-17 15:41:03.0036 - INFO - graphrag.index.operations.embed_text.embed_text - uploading text embeddings batch 1/1 of size 500 to vector store
2025-08-17 15:41:03.0053 - INFO - graphrag.index.operations.embed_text.strategies.openai - embedding 6 inputs via 6 snippets using 1 batches. max_batch_size=16, batch_max_tokens=8191
2025-08-17 15:41:04.0605 - INFO - graphrag.logger.progress - generate embeddings progress: 1/1
2025-08-17 15:41:04.0631 - INFO - graphrag.index.operations.embed_text.embed_text - using vector store lancedb with container_name default for embedding community.full_content: default-community-full_content
2025-08-17 15:41:04.0634 - INFO - graphrag.index.operations.embed_text.embed_text - uploading text embeddings batch 1/1 of size 500 to vector store
2025-08-17 15:41:04.0635 - INFO - graphrag.index.operations.embed_text.strategies.openai - embedding 1 inputs via 1 snippets using 1 batches. max_batch_size=16, batch_max_tokens=8191
2025-08-17 15:41:05.0179 - INFO - graphrag.logger.progress - generate embeddings progress: 1/1
2025-08-17 15:41:05.0186 - INFO - graphrag.index.operations.embed_text.embed_text - using vector store lancedb with container_name default for embedding text_unit.text: default-text_unit-text
2025-08-17 15:41:05.0188 - INFO - graphrag.index.operations.embed_text.embed_text - uploading text embeddings batch 1/1 of size 500 to vector store
2025-08-17 15:41:05.0200 - INFO - graphrag.index.operations.embed_text.strategies.openai - embedding 24 inputs via 24 snippets using 4 batches. max_batch_size=16, batch_max_tokens=8191
2025-08-17 15:41:05.0935 - INFO - graphrag.logger.progress - generate embeddings progress: 1/4
2025-08-17 15:41:06.0217 - INFO - graphrag.logger.progress - generate embeddings progress: 2/4
2025-08-17 15:41:06.0641 - INFO - graphrag.logger.progress - generate embeddings progress: 3/4
2025-08-17 15:41:08.0120 - INFO - graphrag.logger.progress - generate embeddings progress: 4/4
2025-08-17 15:41:08.0130 - INFO - graphrag.index.workflows.generate_text_embeddings - Workflow completed: generate_text_embeddings
2025-08-17 15:41:08.0131 - INFO - graphrag.api.index - Workflow generate_text_embeddings completed successfully
2025-08-17 15:41:08.0147 - INFO - graphrag.index.run.run_pipeline - Indexing pipeline complete.
2025-08-17 15:41:08.0149 - INFO - graphrag.cli.index - All workflows completed successfully.
2025-08-17 15:46:15.0906 - INFO - graphrag.cli.query - Global Search Response:
The narrative surrounding Green Apple Manor is rich with themes that delve into community centrality, intergenerational family dynamics, and the influence of corporate ownership on residential life. Each of these themes is interwoven to create a complex tapestry that reflects the multifaceted nature of living within a community space that is both a home and a subject of corporate policy.

### Community Centrality

Green Apple Manor stands as a pivotal element within its community, serving not just as a residential complex but as a social hub that fosters connections among its inhabitants. The renovation and leasing efforts by Bolton Real Estate have transformed it into a physical and social cornerstone, bridging various community entities and enhancing the communal living experience. This theme underscores the importance of shared spaces in bringing together diverse groups of people, facilitating interactions that enrich the community fabric [Data: Reports (0)].

### Intergenerational Family Dynamics

The story of Geoffrey Webber and his granddaughter Amelia living together at Green Apple Manor brings to light the theme of intergenerational family dynamics. Their cohabitation exemplifies the daily life and mutual dependence that can exist within a cross-generational household. This aspect of the narrative highlights the challenges and rewards of living arrangements that span different age groups, offering insights into the complexities of family life that blends tradition with modernity. The portrayal of their relationship provides a microcosm of the broader societal shifts towards more inclusive and diverse family structures [Data: Reports (0)].

### Influence of Corporate Ownership

The role of Bolton Real Estate as the owner and manager of Green Apple Manor introduces a critical examination of how corporate policies and practices impact the quality of life for residents and the sustainability of the community at large. This theme explores the balance between profit motives and the welfare of the community, questioning the extent to which corporate entities should be involved in the management of living spaces. The influence of Bolton Real Estate on maintenance and leasing policies serves as a case study for the broader implications of corporate involvement in residential areas, highlighting the potential for both positive and negative outcomes on the community's well-being [Data: Reports (0)].

In summary, the story of Green Apple Manor and its inhabitants offers a nuanced exploration of the themes of community centrality, intergenerational family dynamics, and the influence of corporate ownership. These themes are not only relevant to the characters and setting of this particular narrative but also resonate with broader societal issues, reflecting the complexities of contemporary life.
2025-08-17 15:56:25.0206 - INFO - graphrag.cli.prompt_tune - Logging enabled at E:\ai\AI-Driven-TRPG\backend\rag\logs\logs.txt
2025-08-17 15:56:25.0206 - INFO - graphrag.api.prompt_tune - Chunking documents...
2025-08-17 15:56:25.0206 - INFO - graphrag.storage.file_pipeline_storage - Creating file storage at E:\ai\AI-Driven-TRPG\backend\rag\input
2025-08-17 15:56:25.0206 - INFO - graphrag.index.input.factory - loading input from root_dir=E:\ai\AI-Driven-TRPG\backend\rag\input
2025-08-17 15:56:25.0206 - INFO - graphrag.index.input.factory - Loading Input InputFileType.text
2025-08-17 15:56:25.0206 - INFO - graphrag.storage.file_pipeline_storage - search E:\ai\AI-Driven-TRPG\backend\rag\input for files matching .*\.txt$
2025-08-17 15:56:25.0211 - INFO - graphrag.index.input.util - Found 1 InputFileType.text files, loading 1
2025-08-17 15:56:25.0211 - INFO - graphrag.index.input.util - Total number of unfiltered text rows: 1
2025-08-17 15:56:25.0211 - INFO - graphrag.index.workflows.create_base_text_units - Starting chunking process for 1 documents
2025-08-17 15:56:25.0302 - INFO - graphrag.index.workflows.create_base_text_units - chunker progress:  1/1
2025-08-17 15:56:25.0308 - INFO - graphrag.api.prompt_tune - Retrieving language model configuration...
2025-08-17 15:56:25.0308 - INFO - graphrag.api.prompt_tune - Creating language model...
2025-08-17 15:56:28.0136 - INFO - graphrag.api.prompt_tune - Detecting language...
2025-08-17 15:56:30.0516 - INFO - graphrag.api.prompt_tune - Generating persona...
2025-08-17 15:56:33.0092 - INFO - graphrag.api.prompt_tune - Generating community report ranking description...
2025-08-17 15:56:36.0015 - INFO - graphrag.api.prompt_tune - Generating entity types...
2025-08-17 15:56:47.0477 - INFO - graphrag.api.prompt_tune - Generating entity relationship examples...
2025-08-17 15:57:26.0379 - INFO - graphrag.api.prompt_tune - Generating entity extraction prompt...
2025-08-17 15:57:26.0381 - INFO - graphrag.api.prompt_tune - Generating entity summarization prompt...
2025-08-17 15:57:26.0381 - INFO - graphrag.api.prompt_tune - Generating community reporter role...
2025-08-17 15:57:48.0493 - INFO - graphrag.api.prompt_tune - Generating community summarization prompt...
2025-08-17 15:57:48.0493 - INFO - graphrag.cli.prompt_tune - Writing prompts to E:\ai\AI-Driven-TRPG\backend\prompts
2025-08-17 15:57:48.0497 - INFO - graphrag.cli.prompt_tune - Prompts written to E:\ai\AI-Driven-TRPG\backend\prompts
2025-08-17 16:24:19.0800 - INFO - graphrag.cli.index - Logging enabled at E:\ai\AI-Driven-TRPG\backend\rag\logs\logs.txt
2025-08-17 16:24:21.0400 - INFO - graphrag.index.validate_config - LLM Config Params Validated
2025-08-17 16:24:22.0210 - INFO - graphrag.index.validate_config - Embedding LLM Config Params Validated
2025-08-17 16:24:22.0210 - INFO - graphrag.cli.index - Starting pipeline run. False
2025-08-17 16:24:22.0210 - INFO - graphrag.cli.index - Using default configuration: {
    "root_dir": "E:\\ai\\AI-Driven-TRPG\\backend\\rag",
    "models": {
        "default_chat_model": {
            "api_key": "==== REDACTED ====",
            "auth_type": "api_key",
            "type": "openai_chat",
            "model": "gpt-4o",
            "encoding_model": "o200k_base",
            "api_base": null,
            "api_version": null,
            "deployment_name": null,
            "proxy": null,
            "audience": null,
            "model_supports_json": true,
            "request_timeout": 180.0,
            "tokens_per_minute": "auto",
            "requests_per_minute": "auto",
            "retry_strategy": "native",
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "concurrent_requests": 25,
            "async_mode": "threaded",
            "responses": null,
            "max_tokens": null,
            "temperature": 0,
            "max_completion_tokens": null,
            "reasoning_effort": null,
            "top_p": 1,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0
        },
        "default_embedding_model": {
            "api_key": "==== REDACTED ====",
            "auth_type": "api_key",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "encoding_model": "cl100k_base",
            "api_base": null,
            "api_version": null,
            "deployment_name": null,
            "proxy": null,
            "audience": null,
            "model_supports_json": true,
            "request_timeout": 180.0,
            "tokens_per_minute": null,
            "requests_per_minute": null,
            "retry_strategy": "native",
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "concurrent_requests": 25,
            "async_mode": "threaded",
            "responses": null,
            "max_tokens": null,
            "temperature": 0,
            "max_completion_tokens": null,
            "reasoning_effort": null,
            "top_p": 1,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0
        }
    },
    "input": {
        "storage": {
            "type": "file",
            "base_dir": "E:\\ai\\AI-Driven-TRPG\\backend\\rag\\input",
            "storage_account_blob_url": null,
            "cosmosdb_account_url": null
        },
        "file_type": "text",
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "text_column": "text",
        "title_column": null,
        "metadata": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": "tokens",
        "encoding_model": "cl100k_base",
        "prepend_metadata": false,
        "chunk_size_includes_metadata": false
    },
    "output": {
        "type": "file",
        "base_dir": "E:\\ai\\AI-Driven-TRPG\\backend\\rag\\output",
        "storage_account_blob_url": null,
        "cosmosdb_account_url": null
    },
    "outputs": null,
    "update_index_output": {
        "type": "file",
        "base_dir": "E:\\ai\\AI-Driven-TRPG\\backend\\rag\\update_output",
        "storage_account_blob_url": null,
        "cosmosdb_account_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null,
        "cosmosdb_account_url": null
    },
    "reporting": {
        "type": "file",
        "base_dir": "E:\\ai\\AI-Driven-TRPG\\backend\\rag\\logs",
        "storage_account_blob_url": null
    },
    "vector_store": {
        "default_vector_store": {
            "type": "lancedb",
            "db_uri": "E:\\ai\\AI-Driven-TRPG\\backend\\rag\\output\\lancedb",
            "url": null,
            "audience": null,
            "container_name": "==== REDACTED ====",
            "database_name": null,
            "overwrite": true
        }
    },
    "workflows": null,
    "embed_text": {
        "model_id": "default_embedding_model",
        "vector_store_id": "default_vector_store",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "names": [
            "entity.description",
            "community.full_content",
            "text_unit.text"
        ],
        "strategy": null
    },
    "extract_graph": {
        "model_id": "default_chat_model",
        "prompt": "prompts/extract_graph.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null
    },
    "summarize_descriptions": {
        "model_id": "default_chat_model",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "max_input_tokens": 4000,
        "strategy": null
    },
    "extract_graph_nlp": {
        "normalize_edge_weights": true,
        "text_analyzer": {
            "extractor_type": "regex_english",
            "model_name": "en_core_web_md",
            "max_word_length": 15,
            "word_delimiter": " ",
            "include_named_entities": true,
            "exclude_nouns": [
                "stuff",
                "thing",
                "things",
                "bunch",
                "bit",
                "bits",
                "people",
                "person",
                "okay",
                "hey",
                "hi",
                "hello",
                "laughter",
                "oh"
            ],
            "exclude_entity_tags": [
                "DATE"
            ],
            "exclude_pos_tags": [
                "DET",
                "PRON",
                "INTJ",
                "X"
            ],
            "noun_phrase_tags": [
                "PROPN",
                "NOUNS"
            ],
            "noun_phrase_grammars": {
                "PROPN,PROPN": "PROPN",
                "NOUN,NOUN": "NOUNS",
                "NOUNS,NOUN": "NOUNS",
                "ADJ,ADJ": "ADJ",
                "ADJ,NOUN": "NOUNS"
            }
        },
        "concurrent_requests": 25
    },
    "prune_graph": {
        "min_node_freq": 2,
        "max_node_freq_std": null,
        "min_node_degree": 1,
        "max_node_degree_std": null,
        "min_edge_weight_pct": 40.0,
        "remove_ego_nodes": true,
        "lcc_only": false
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "use_lcc": true,
        "seed": 3735928559
    },
    "extract_claims": {
        "enabled": false,
        "model_id": "default_chat_model",
        "prompt": "prompts/extract_claims.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null
    },
    "community_reports": {
        "model_id": "default_chat_model",
        "graph_prompt": "prompts/community_report_graph.txt",
        "text_prompt": "prompts/community_report_text.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "embed_graph": {
        "enabled": false,
        "dimensions": 1536,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "use_lcc": true
    },
    "umap": {
        "enabled": false
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false,
        "raw_graph": false
    },
    "local_search": {
        "prompt": "prompts/local_search_system_prompt.txt",
        "chat_model_id": "default_chat_model",
        "embedding_model_id": "default_embedding_model",
        "text_unit_prop": 0.5,
        "community_prop": 0.15,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "max_context_tokens": 12000
    },
    "global_search": {
        "map_prompt": "prompts/global_search_map_system_prompt.txt",
        "reduce_prompt": "prompts/global_search_reduce_system_prompt.txt",
        "chat_model_id": "default_chat_model",
        "knowledge_prompt": "prompts/global_search_knowledge_system_prompt.txt",
        "max_context_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_length": 1000,
        "reduce_max_length": 2000,
        "dynamic_search_threshold": 1,
        "dynamic_search_keep_parent": false,
        "dynamic_search_num_repeats": 1,
        "dynamic_search_use_summary": false,
        "dynamic_search_max_level": 2
    },
    "drift_search": {
        "prompt": "prompts/drift_search_system_prompt.txt",
        "reduce_prompt": "prompts/drift_search_reduce_prompt.txt",
        "chat_model_id": "default_chat_model",
        "embedding_model_id": "default_embedding_model",
        "data_max_tokens": 12000,
        "reduce_max_tokens": null,
        "reduce_temperature": 0,
        "reduce_max_completion_tokens": null,
        "concurrency": 32,
        "drift_k_followups": 20,
        "primer_folds": 5,
        "primer_llm_max_tokens": 12000,
        "n_depth": 3,
        "local_search_text_unit_prop": 0.9,
        "local_search_community_prop": 0.1,
        "local_search_top_k_mapped_entities": 10,
        "local_search_top_k_relationships": 10,
        "local_search_max_data_tokens": 12000,
        "local_search_temperature": 0,
        "local_search_top_p": 1,
        "local_search_n": 1,
        "local_search_llm_max_gen_tokens": null,
        "local_search_llm_max_gen_completion_tokens": null
    },
    "basic_search": {
        "prompt": "prompts/basic_search_system_prompt.txt",
        "chat_model_id": "default_chat_model",
        "embedding_model_id": "default_embedding_model",
        "k": 10,
        "max_context_tokens": 12000
    }
}
2025-08-17 16:24:22.0212 - INFO - graphrag.api.index - Initializing indexing pipeline...
2025-08-17 16:24:22.0212 - INFO - graphrag.index.workflows.factory - Creating pipeline with workflows: ['load_input_documents', 'create_base_text_units', 'create_final_documents', 'extract_graph', 'finalize_graph', 'extract_covariates', 'create_communities', 'create_final_text_units', 'create_community_reports', 'generate_text_embeddings']
2025-08-17 16:24:22.0212 - INFO - graphrag.storage.file_pipeline_storage - Creating file storage at E:\ai\AI-Driven-TRPG\backend\rag\input
2025-08-17 16:24:22.0212 - INFO - graphrag.storage.file_pipeline_storage - Creating file storage at E:\ai\AI-Driven-TRPG\backend\rag\output
2025-08-17 16:24:22.0212 - INFO - graphrag.index.run.run_pipeline - Running standard indexing.
2025-08-17 16:24:22.0212 - INFO - graphrag.index.run.run_pipeline - Executing pipeline...
2025-08-17 16:24:22.0212 - INFO - graphrag.index.input.factory - loading input from root_dir=E:\ai\AI-Driven-TRPG\backend\rag\input
2025-08-17 16:24:22.0212 - INFO - graphrag.index.input.factory - Loading Input InputFileType.text
2025-08-17 16:24:22.0212 - INFO - graphrag.storage.file_pipeline_storage - search E:\ai\AI-Driven-TRPG\backend\rag\input for files matching .*\.txt$
2025-08-17 16:24:22.0221 - INFO - graphrag.index.input.util - Found 1 InputFileType.text files, loading 1
2025-08-17 16:24:22.0221 - INFO - graphrag.index.input.util - Total number of unfiltered text rows: 1
2025-08-17 16:24:22.0222 - INFO - graphrag.index.workflows.load_input_documents - Final # of rows loaded: 1
2025-08-17 16:24:22.0228 - INFO - graphrag.api.index - Workflow load_input_documents completed successfully
2025-08-17 16:24:22.0234 - INFO - graphrag.index.workflows.create_base_text_units - Workflow started: create_base_text_units
2025-08-17 16:24:22.0235 - INFO - graphrag.utils.storage - reading table from storage: documents.parquet
2025-08-17 16:24:22.0242 - INFO - graphrag.index.workflows.create_base_text_units - Starting chunking process for 1 documents
2025-08-17 16:24:22.0252 - INFO - graphrag.index.workflows.create_base_text_units - chunker progress:  1/1
2025-08-17 16:24:22.0258 - INFO - graphrag.index.workflows.create_base_text_units - Workflow completed: create_base_text_units
2025-08-17 16:24:22.0258 - INFO - graphrag.api.index - Workflow create_base_text_units completed successfully
2025-08-17 16:24:22.0262 - INFO - graphrag.index.workflows.create_final_documents - Workflow started: create_final_documents
2025-08-17 16:24:22.0264 - INFO - graphrag.utils.storage - reading table from storage: documents.parquet
2025-08-17 16:24:22.0267 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2025-08-17 16:24:22.0271 - INFO - graphrag.index.workflows.create_final_documents - Workflow completed: create_final_documents
2025-08-17 16:24:22.0271 - INFO - graphrag.api.index - Workflow create_final_documents completed successfully
2025-08-17 16:24:22.0282 - INFO - graphrag.index.workflows.extract_graph - Workflow started: extract_graph
2025-08-17 16:24:22.0284 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2025-08-17 16:24:41.0000 - INFO - graphrag.logger.progress - extract graph progress: 1/24
2025-08-17 16:25:44.0090 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 3013. Please try again in 6.026s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:25:44.0090 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 3013. Please try again in 6.026s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:25:44.0090 - INFO - graphrag.logger.progress - extract graph progress: 2/24
2025-08-17 16:25:57.0589 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 3661. Please try again in 7.322s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:25:57.0590 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 3661. Please try again in 7.322s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:25:57.0590 - INFO - graphrag.logger.progress - extract graph progress: 3/24
2025-08-17 16:25:58.0237 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 3680. Please try again in 7.36s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:25:58.0238 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 3680. Please try again in 7.36s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:25:58.0239 - INFO - graphrag.logger.progress - extract graph progress: 4/24
2025-08-17 16:26:01.0584 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 3765. Please try again in 7.53s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:01.0586 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 3765. Please try again in 7.53s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:01.0586 - INFO - graphrag.logger.progress - extract graph progress: 5/24
2025-08-17 16:26:01.0635 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 3652. Please try again in 7.304s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:01.0635 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 3652. Please try again in 7.304s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:01.0635 - INFO - graphrag.logger.progress - extract graph progress: 6/24
2025-08-17 16:26:02.0431 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 3789. Please try again in 7.578s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:02.0432 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 3789. Please try again in 7.578s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:02.0433 - INFO - graphrag.logger.progress - extract graph progress: 7/24
2025-08-17 16:26:02.0473 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 3812. Please try again in 7.624s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:02.0475 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 3812. Please try again in 7.624s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:02.0476 - INFO - graphrag.logger.progress - extract graph progress: 8/24
2025-08-17 16:26:03.0230 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 3921. Please try again in 7.842s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:03.0232 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 3921. Please try again in 7.842s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:03.0232 - INFO - graphrag.logger.progress - extract graph progress: 9/24
2025-08-17 16:26:03.0780 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 3947. Please try again in 7.894s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:03.0780 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 3947. Please try again in 7.894s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:03.0780 - INFO - graphrag.logger.progress - extract graph progress: 10/24
2025-08-17 16:26:03.0805 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 3830. Please try again in 7.66s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:03.0807 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 3830. Please try again in 7.66s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:03.0807 - INFO - graphrag.logger.progress - extract graph progress: 11/24
2025-08-17 16:26:04.0430 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 3890. Please try again in 7.78s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:04.0431 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 3890. Please try again in 7.78s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:04.0431 - INFO - graphrag.logger.progress - extract graph progress: 12/24
2025-08-17 16:26:04.0898 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 3898. Please try again in 7.796s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:04.0901 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 3898. Please try again in 7.796s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:04.0901 - INFO - graphrag.logger.progress - extract graph progress: 13/24
2025-08-17 16:26:06.0204 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 3916. Please try again in 7.832s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:06.0204 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 3916. Please try again in 7.832s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:06.0204 - INFO - graphrag.logger.progress - extract graph progress: 14/24
2025-08-17 16:26:06.0737 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 3948. Please try again in 7.896s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:06.0737 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 3948. Please try again in 7.896s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:06.0740 - INFO - graphrag.logger.progress - extract graph progress: 15/24
2025-08-17 16:26:07.0193 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 3896. Please try again in 7.791s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:07.0193 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 3896. Please try again in 7.791s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:07.0193 - INFO - graphrag.logger.progress - extract graph progress: 16/24
2025-08-17 16:26:07.0193 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 3960. Please try again in 7.92s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:07.0200 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 3960. Please try again in 7.92s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:07.0200 - INFO - graphrag.logger.progress - extract graph progress: 17/24
2025-08-17 16:26:07.0700 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 4019. Please try again in 8.038s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:07.0700 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 4019. Please try again in 8.038s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:07.0700 - INFO - graphrag.logger.progress - extract graph progress: 18/24
2025-08-17 16:26:08.0125 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 3969. Please try again in 7.938s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:08.0125 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 3969. Please try again in 7.938s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:08.0127 - INFO - graphrag.logger.progress - extract graph progress: 19/24
2025-08-17 16:26:08.0415 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 3956. Please try again in 7.912s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:08.0417 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 3956. Please try again in 7.912s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:08.0417 - INFO - graphrag.logger.progress - extract graph progress: 20/24
2025-08-17 16:26:08.0887 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 3859. Please try again in 7.717s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:08.0889 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 3859. Please try again in 7.717s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:08.0890 - INFO - graphrag.logger.progress - extract graph progress: 21/24
2025-08-17 16:26:10.0248 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 4063. Please try again in 8.125s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:10.0250 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 4063. Please try again in 8.125s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:10.0250 - INFO - graphrag.logger.progress - extract graph progress: 22/24
2025-08-17 16:26:13.0630 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 4160. Please try again in 8.32s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:13.0630 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 4160. Please try again in 8.32s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:13.0634 - INFO - graphrag.logger.progress - extract graph progress: 23/24
2025-08-17 16:26:14.0110 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 3942. Please try again in 7.883s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:14.0110 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 84, in achat
    response = await self.model(prompt, history=history, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 78, in invoke
    return await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 3942. Please try again in 7.883s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:14.0110 - INFO - graphrag.logger.progress - extract graph progress: 24/24
2025-08-17 16:26:14.0142 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 1/19
2025-08-17 16:26:14.0143 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 2/19
2025-08-17 16:26:14.0144 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 3/19
2025-08-17 16:26:14.0144 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 4/19
2025-08-17 16:26:14.0144 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 5/19
2025-08-17 16:26:14.0144 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 6/19
2025-08-17 16:26:14.0144 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 7/19
2025-08-17 16:26:14.0144 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 8/19
2025-08-17 16:26:14.0144 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 9/19
2025-08-17 16:26:14.0144 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 10/19
2025-08-17 16:26:14.0145 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 11/19
2025-08-17 16:26:14.0145 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 12/19
2025-08-17 16:26:14.0145 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 13/19
2025-08-17 16:26:14.0145 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 14/19
2025-08-17 16:26:14.0146 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 15/19
2025-08-17 16:26:14.0146 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 16/19
2025-08-17 16:26:14.0146 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 17/19
2025-08-17 16:26:14.0146 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 18/19
2025-08-17 16:26:14.0146 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 19/19
2025-08-17 16:26:14.0150 - INFO - graphrag.index.workflows.extract_graph - Workflow completed: extract_graph
2025-08-17 16:26:14.0150 - INFO - graphrag.api.index - Workflow extract_graph completed successfully
2025-08-17 16:26:14.0163 - INFO - graphrag.index.workflows.finalize_graph - Workflow started: finalize_graph
2025-08-17 16:26:14.0164 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2025-08-17 16:26:14.0167 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2025-08-17 16:26:14.0183 - INFO - graphrag.index.workflows.finalize_graph - Workflow completed: finalize_graph
2025-08-17 16:26:14.0183 - INFO - graphrag.api.index - Workflow finalize_graph completed successfully
2025-08-17 16:26:14.0191 - INFO - graphrag.index.workflows.extract_covariates - Workflow started: extract_covariates
2025-08-17 16:26:14.0191 - INFO - graphrag.index.workflows.extract_covariates - Workflow completed: extract_covariates
2025-08-17 16:26:14.0191 - INFO - graphrag.api.index - Workflow extract_covariates completed successfully
2025-08-17 16:26:14.0191 - INFO - graphrag.index.workflows.create_communities - Workflow started: create_communities
2025-08-17 16:26:14.0192 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2025-08-17 16:26:14.0196 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2025-08-17 16:26:14.0217 - INFO - graphrag.index.workflows.create_communities - Workflow completed: create_communities
2025-08-17 16:26:14.0218 - INFO - graphrag.api.index - Workflow create_communities completed successfully
2025-08-17 16:26:14.0224 - INFO - graphrag.index.workflows.create_final_text_units - Workflow started: create_final_text_units
2025-08-17 16:26:14.0224 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2025-08-17 16:26:14.0228 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2025-08-17 16:26:14.0234 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2025-08-17 16:26:14.0249 - INFO - graphrag.index.workflows.create_final_text_units - Workflow completed: create_final_text_units
2025-08-17 16:26:14.0249 - INFO - graphrag.api.index - Workflow create_final_text_units completed successfully
2025-08-17 16:26:14.0256 - INFO - graphrag.index.workflows.create_community_reports - Workflow started: create_community_reports
2025-08-17 16:26:14.0256 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2025-08-17 16:26:14.0261 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2025-08-17 16:26:14.0266 - INFO - graphrag.utils.storage - reading table from storage: communities.parquet
2025-08-17 16:26:14.0273 - INFO - graphrag.index.operations.summarize_communities.graph_context.context_builder - Number of nodes at level=0 => 5
2025-08-17 16:26:55.0933 - ERROR - graphrag.language_model.providers.fnllm.utils - Error Invoking LLM
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 77, in invoke
    return await this.invoke_json(delegate, prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 96, in invoke_json
    return await self.try_receive_json(delegate, prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 112, in try_receive_json
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 1921. Please try again in 3.842s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:55.0933 - ERROR - graphrag.index.operations.summarize_communities.community_reports_extractor - error generating community report
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\summarize_communities\community_reports_extractor.py", line 80, in __call__
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 82, in achat
    response = await self.model(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 77, in invoke
    return await this.invoke_json(delegate, prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 96, in invoke_json
    return await self.try_receive_json(delegate, prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 112, in try_receive_json
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 1921. Please try again in 3.842s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:55.0933 - ERROR - graphrag.index.operations.summarize_communities.strategies - Community Report Extraction Error
Traceback (most recent call last):
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\index\operations\summarize_communities\community_reports_extractor.py", line 80, in __call__
    response = await self._model.achat(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 82, in achat
    response = await self.model(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_chat_llm.py", line 94, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\services\openai_tools_parsing.py", line 141, in __call__
    return await self._delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 77, in invoke
    return await this.invoke_json(delegate, prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 96, in invoke_json
    return await self.try_receive_json(delegate, prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\json.py", line 112, in try_receive_json
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\fnllm\openai\llm\openai_text_chat_llm.py", line 173, in _execute_llm
    raw_response = await self._client.chat.completions.with_raw_response.create(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\resources\chat\completions\completions.py", line 2589, in create
    return await self._post(
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "C:\Users\alian\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-wpbgCq87uriq8MOE6N6oxncI on tokens per min (TPM): Limit 30000, Used 30000, Requested 1921. Please try again in 3.842s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2025-08-17 16:26:55.0933 - WARNING - graphrag.index.operations.summarize_communities.strategies - No report found for community: 1.0
2025-08-17 16:26:55.0933 - INFO - graphrag.logger.progress - level 0 summarize communities progress: 1/2
2025-08-17 16:27:23.0201 - INFO - graphrag.logger.progress - level 0 summarize communities progress: 2/2
2025-08-17 16:27:23.0210 - INFO - graphrag.index.workflows.create_community_reports - Workflow completed: create_community_reports
2025-08-17 16:27:23.0210 - INFO - graphrag.api.index - Workflow create_community_reports completed successfully
2025-08-17 16:27:23.0215 - INFO - graphrag.index.workflows.generate_text_embeddings - Workflow started: generate_text_embeddings
2025-08-17 16:27:23.0216 - INFO - graphrag.utils.storage - reading table from storage: documents.parquet
2025-08-17 16:27:23.0219 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2025-08-17 16:27:23.0223 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2025-08-17 16:27:23.0226 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2025-08-17 16:27:23.0229 - INFO - graphrag.utils.storage - reading table from storage: community_reports.parquet
2025-08-17 16:27:23.0235 - INFO - graphrag.index.workflows.generate_text_embeddings - Creating embeddings
2025-08-17 16:27:23.0235 - INFO - graphrag.index.operations.embed_text.embed_text - using vector store lancedb with container_name default for embedding entity.description: default-entity-description
2025-08-17 16:27:23.0245 - INFO - graphrag.index.operations.embed_text.embed_text - uploading text embeddings batch 1/1 of size 500 to vector store
2025-08-17 16:27:23.0263 - INFO - graphrag.index.operations.embed_text.strategies.openai - embedding 10 inputs via 10 snippets using 1 batches. max_batch_size=16, batch_max_tokens=8191
2025-08-17 16:27:24.0604 - INFO - graphrag.logger.progress - generate embeddings progress: 1/1
2025-08-17 16:27:24.0624 - INFO - graphrag.index.operations.embed_text.embed_text - using vector store lancedb with container_name default for embedding community.full_content: default-community-full_content
2025-08-17 16:27:24.0626 - INFO - graphrag.index.operations.embed_text.embed_text - uploading text embeddings batch 1/1 of size 500 to vector store
2025-08-17 16:27:24.0626 - INFO - graphrag.index.operations.embed_text.strategies.openai - embedding 1 inputs via 1 snippets using 1 batches. max_batch_size=16, batch_max_tokens=8191
2025-08-17 16:27:25.0120 - INFO - graphrag.logger.progress - generate embeddings progress: 1/1
2025-08-17 16:27:25.0120 - INFO - graphrag.index.operations.embed_text.embed_text - using vector store lancedb with container_name default for embedding text_unit.text: default-text_unit-text
2025-08-17 16:27:25.0129 - INFO - graphrag.index.operations.embed_text.embed_text - uploading text embeddings batch 1/1 of size 500 to vector store
2025-08-17 16:27:25.0140 - INFO - graphrag.index.operations.embed_text.strategies.openai - embedding 24 inputs via 24 snippets using 4 batches. max_batch_size=16, batch_max_tokens=8191
2025-08-17 16:27:25.0147 - INFO - graphrag.logger.progress - generate embeddings progress: 1/4
2025-08-17 16:27:25.0150 - INFO - graphrag.logger.progress - generate embeddings progress: 2/4
2025-08-17 16:27:25.0152 - INFO - graphrag.logger.progress - generate embeddings progress: 3/4
2025-08-17 16:27:25.0156 - INFO - graphrag.logger.progress - generate embeddings progress: 4/4
2025-08-17 16:27:25.0165 - INFO - graphrag.index.workflows.generate_text_embeddings - Workflow completed: generate_text_embeddings
2025-08-17 16:27:25.0165 - INFO - graphrag.api.index - Workflow generate_text_embeddings completed successfully
2025-08-17 16:27:25.0184 - INFO - graphrag.index.run.run_pipeline - Indexing pipeline complete.
2025-08-17 16:27:25.0188 - INFO - graphrag.cli.index - All workflows completed successfully.
2025-08-17 16:30:54.0510 - WARNING - graphrag.query.indexer_adapters - Missing reports for communities: [1]
2025-08-17 16:31:04.0882 - INFO - graphrag.cli.query - Global Search Response:
The story in the '' module is rich with themes that revolve around supernatural challenges and the setting of a 1920s Lovecraftian world. Here are the top themes identified:

### Supernatural Challenges

A central theme of the story is the supernatural challenges faced by the investigators. These challenges are not only physical but also mental, testing the endurance and resilience of the characters. The investigators must navigate through these supernatural events, which are integral to the narrative, to survive and complete their mission [Data: Reports (0)].

### 1920s Lovecraftian Setting

The setting of the '' module is another significant theme. It is based in a 1920s Lovecraftian world, which provides a backdrop filled with supernatural occurrences. This setting creates a dangerous and challenging environment for the investigators, adding depth and complexity to the story [Data: Reports (0)].

### Impact of Supernatural Events

The impact of supernatural events on the investigators is a major theme. These events pose significant threats to their mental and physical well-being. The investigators must employ their skills and strategies to overcome these challenges, which are crucial for their survival and the successful completion of their mission [Data: Reports (0)].

### Threat of Storms

The threat of storms is another key theme in the story. Storms present a significant danger to the investigators, heightening the tension and challenge of the module. The presence of storms requires the investigators to make quick decisions and react under extreme conditions, further testing their abilities and resolve [Data: Reports (0)].

These themes collectively create a narrative that is both engaging and challenging, requiring the investigators to confront and overcome a series of formidable obstacles.
